#!/usr/bin/env python3
"""
Script pour r√©g√©n√©rer les embeddings du document BMI.
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path pour importer les modules de l'app
sys.path.append(str(Path(__file__).parent.parent))

from app.core.database import get_db
from app.models import Document, DocumentChunk
from app.services.document_service import DocumentProcessor
from app.services.vector_service import VectorService
from app.services.embedding_service import EmbeddingService
from sqlalchemy import select
from loguru import logger


async def regenerate_bmi_embeddings():
    """R√©g√©n√®re les embeddings du document BMI."""
    
    try:
        # Initialiser les services
        vector_service = VectorService()
        embedding_service = EmbeddingService()
        document_processor = DocumentProcessor()
        
        await vector_service.initialize()
        
        # Trouver le document BMI
        async for db_session in get_db():
            stmt = select(Document).where(Document.original_filename == "Information sur BMI.pdf")
            result = await db_session.execute(stmt)
            document = result.scalar_one_or_none()
            
            if not document:
                logger.error("‚ùå Document BMI non trouv√©")
                return False
            
            logger.info(f"üìÑ Document BMI trouv√©: {document.id}")
            
            # R√©cup√©rer les chunks du document
            stmt = select(DocumentChunk).where(DocumentChunk.document_id == document.id)
            result = await db_session.execute(stmt)
            chunks = result.scalars().all()
            
            logger.info(f"üì¶ Nombre de chunks √† traiter: {len(chunks)}")
            
            # Supprimer les anciens embeddings de la base vectorielle
            logger.info("üóëÔ∏è Suppression des anciens embeddings...")
            try:
                results = vector_service.collection.get(
                    where={"document_id": document.id}
                )
                
                if results["ids"]:
                    vector_service.collection.delete(ids=results["ids"])
                    logger.info(f"‚úÖ Supprim√© {len(results['ids'])} anciens embeddings")
                else:
                    logger.info("‚ÑπÔ∏è Aucun ancien embedding √† supprimer")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lors de la suppression: {str(e)}")
            
            # R√©g√©n√©rer les embeddings
            logger.info("üß† R√©g√©n√©ration des embeddings...")
            
            chunks_text = [chunk.content for chunk in chunks]
            embeddings = await embedding_service.generate_embeddings(chunks_text)
            
            if not embeddings:
                logger.error("‚ùå √âchec de la g√©n√©ration des embeddings")
                return False
            
            logger.info(f"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s")
            
            # Pr√©parer les m√©tadonn√©es
            metadata_list = []
            for i, chunk in enumerate(chunks):
                metadata = {
                    "document_id": document.id,
                    "chunk_index": i,
                    "filename": document.original_filename,
                    "file_type": document.file_type.value,
                    "chunk_length": len(chunk.content),
                    "created_at": chunk.created_at.isoformat() if chunk.created_at else None,
                    "keywords": document.keywords or "",
                    "chunk_type": "regular",
                    "has_questions": False,
                    "has_answers": False,
                    "confidence_score": 1.0,
                    "word_count": len(chunk.content.split())
                }
                metadata_list.append(metadata)
            
            # Ajouter les nouveaux embeddings √† la base vectorielle
            logger.info("üíæ Ajout des nouveaux embeddings...")
            await vector_service.add_document_chunks(
                document_id=document.id,
                chunks=chunks_text,
                embeddings=embeddings,
                metadata=metadata_list
            )
            
            logger.info("‚úÖ Embeddings ajout√©s avec succ√®s")
            
            # V√©rifier que les embeddings sont bien stock√©s
            logger.info("üîç V√©rification des nouveaux embeddings...")
            results = vector_service.collection.get(
                where={"document_id": document.id}
            )
            
            if results["ids"]:
                logger.info(f"‚úÖ {len(results['ids'])} embeddings trouv√©s")
                for i, (chunk_id, metadata) in enumerate(zip(results["ids"], results["metadatas"])):
                    logger.info(f"  {i+1}. ID: {chunk_id}")
                    logger.info(f"     Filename: {metadata.get('filename', 'Inconnu')}")
                    logger.info(f"     Chunk index: {metadata.get('chunk_index', 'Inconnu')}")
                    
                    # V√©rifier l'embedding
                    if "embeddings" in results and results["embeddings"]:
                        embedding = results["embeddings"][i]
                        logger.info(f"     Embedding: {len(embedding)} dimensions")
                    else:
                        logger.warning(f"     ‚ö†Ô∏è Pas d'embedding")
            else:
                logger.warning("‚ö†Ô∏è Aucun embedding trouv√© apr√®s ajout")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©g√©n√©ration: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


if __name__ == "__main__":
    logger.info("üöÄ R√©g√©n√©ration des embeddings BMI...")
    
    success = asyncio.run(regenerate_bmi_embeddings())
    
    if success:
        logger.info("‚úÖ R√©g√©n√©ration termin√©e")
    else:
        logger.error("‚ùå √âchec de la r√©g√©n√©ration")
        sys.exit(1) 