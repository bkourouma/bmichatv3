#!/usr/bin/env python3
"""
Script pour recr√©er compl√®tement la collection ChromaDB.
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path pour importer les modules de l'app
sys.path.append(str(Path(__file__).parent.parent))

from app.services.vector_service import VectorService
from app.services.embedding_service import EmbeddingService
from app.core.database import get_db
from app.models import Document, DocumentChunk
from sqlalchemy import select
from loguru import logger


async def recreate_chromadb():
    """Recr√©e compl√®tement la collection ChromaDB."""
    
    try:
        # Initialiser les services
        vector_service = VectorService()
        embedding_service = EmbeddingService()
        
        logger.info("üîÑ Recr√©ation compl√®te de ChromaDB...")
        
        # 1. Supprimer la collection existante
        logger.info("\nüóëÔ∏è Suppression de la collection existante...")
        
        try:
            await vector_service.initialize()
            
            # Supprimer tous les documents
            all_results = vector_service.collection.get()
            if all_results["ids"]:
                vector_service.collection.delete(ids=all_results["ids"])
                logger.info(f"‚úÖ Supprim√© {len(all_results['ids'])} documents")
            
            # Supprimer la collection
            vector_service.client.delete_collection("bmi_documents")
            logger.info("‚úÖ Collection supprim√©e")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la suppression: {str(e)}")
        
        # 2. Recr√©er la collection
        logger.info("\nüÜï Recr√©ation de la collection...")
        
        try:
            vector_service.collection = vector_service.client.create_collection(
                name="bmi_documents",
                metadata={"description": "BMI Chat document embeddings"}
            )
            logger.info("‚úÖ Collection recr√©√©e")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recr√©ation: {str(e)}")
            return False
        
        # 3. R√©cup√©rer tous les documents de la base de donn√©es
        logger.info("\nüìö R√©cup√©ration des documents...")
        
        documents_to_process = []
        
        async for db_session in get_db():
            # R√©cup√©rer tous les documents trait√©s
            stmt = select(Document).where(Document.status == "PROCESSED")
            result = await db_session.execute(stmt)
            documents = result.scalars().all()
            
            logger.info(f"üìÑ {len(documents)} documents trait√©s trouv√©s")
            
            for document in documents:
                # R√©cup√©rer les chunks du document
                stmt = select(DocumentChunk).where(DocumentChunk.document_id == document.id)
                result = await db_session.execute(stmt)
                chunks = result.scalars().all()
                
                if chunks:
                    documents_to_process.append({
                        "document": document,
                        "chunks": chunks
                    })
                    logger.info(f"  - {document.original_filename}: {len(chunks)} chunks")
        
        # 4. Traiter chaque document
        logger.info(f"\nüß† Traitement de {len(documents_to_process)} documents...")
        
        for doc_info in documents_to_process:
            document = doc_info["document"]
            chunks = doc_info["chunks"]
            
            logger.info(f"\nüìÑ Traitement de {document.original_filename} ({len(chunks)} chunks)")
            
            # Extraire les contenus
            contents = [chunk.content for chunk in chunks]
            
            # G√©n√©rer les embeddings
            logger.info("  üß† G√©n√©ration des embeddings...")
            embeddings = await embedding_service.generate_embeddings(contents)
            
            if not embeddings:
                logger.error(f"  ‚ùå √âchec de la g√©n√©ration des embeddings pour {document.original_filename}")
                continue
            
            logger.info(f"  ‚úÖ {len(embeddings)} embeddings g√©n√©r√©s")
            
            # Pr√©parer les m√©tadonn√©es
            metadata_list = []
            for i, chunk in enumerate(chunks):
                metadata = {
                    "document_id": document.id,
                    "chunk_index": i,
                    "filename": document.original_filename,
                    "file_type": document.file_type.value,
                    "chunk_length": len(chunk.content),
                    "created_at": chunk.created_at.isoformat() if chunk.created_at else None,
                    "keywords": document.keywords or "",
                    "chunk_type": "regular",
                    "has_questions": False,
                    "has_answers": False,
                    "confidence_score": 1.0,
                    "word_count": len(chunk.content.split())
                }
                metadata_list.append(metadata)
            
            # Ajouter √† ChromaDB
            try:
                await vector_service.add_document_chunks(
                    document_id=document.id,
                    chunks=contents,
                    embeddings=embeddings,
                    metadata=metadata_list
                )
                logger.info(f"  ‚úÖ Embeddings ajout√©s pour {document.original_filename}")
                
            except Exception as e:
                logger.error(f"  ‚ùå Erreur lors de l'ajout des embeddings: {str(e)}")
        
        # 5. V√©rifier la nouvelle collection
        logger.info("\nüîç V√©rification de la nouvelle collection...")
        
        try:
            count = vector_service.collection.count()
            logger.info(f"üìä Nombre total de documents: {count}")
            
            # V√©rifier sp√©cifiquement les chunks BMI
            bmi_results = vector_service.collection.get(
                where={"document_id": "99eee4da-b44e-49dd-b553-a862b83c8ccd"}
            )
            
            if bmi_results["ids"]:
                logger.info(f"‚úÖ {len(bmi_results['ids'])} chunks BMI trouv√©s")
                
                for i, chunk_id in enumerate(bmi_results["ids"]):
                    if "embeddings" in bmi_results and i < len(bmi_results["embeddings"]):
                        embedding = bmi_results["embeddings"][i]
                        if embedding is not None and len(embedding) > 0:
                            logger.info(f"  ‚úÖ Chunk {chunk_id}: {len(embedding)} dimensions")
                        else:
                            logger.warning(f"  ‚ö†Ô∏è Chunk {chunk_id}: embedding invalide")
                    else:
                        logger.warning(f"  ‚ö†Ô∏è Chunk {chunk_id}: pas d'embedding")
            else:
                logger.warning("‚ö†Ô∏è Aucun chunk BMI trouv√©")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la v√©rification: {str(e)}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la recr√©ation: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


if __name__ == "__main__":
    logger.info("üöÄ D√©but de la recr√©ation de ChromaDB...")
    
    success = asyncio.run(recreate_chromadb())
    
    if success:
        logger.info("‚úÖ Recr√©ation termin√©e")
    else:
        logger.error("‚ùå √âchec de la recr√©ation")
        sys.exit(1) 